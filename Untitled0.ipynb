{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPOSGTBkl/SKEwywTiDSci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skaurl/HAAFOR-Challenge-2020/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQwVZNuyeIFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aJpQZYmeXlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training.csv', encoding='cp949')\n",
        "\n",
        "print(df.dtypes)\n",
        "\n",
        "print('개수 :', len(df))\n",
        "print('최대 길이 :', max(len(l) for l in df['BEFORE_BODY']))\n",
        "print('평균 길이 :', round(sum(map(len, df['BEFORE_BODY'])) / len(df['BEFORE_BODY'])))\n",
        "\n",
        "plt.hist([len(s) for s in df['BEFORE_BODY']], bins=20)\n",
        "plt.xlim(0,1500)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot([len(s) for s in df['BEFORE_BODY']], shade=True)\n",
        "plt.title('Kernel Density Estimation')\n",
        "plt.show()\n",
        "\n",
        "print('개수 :', len(df))\n",
        "print('최대 길이 :', max(len(l) for l in df['AFTER_BODY']))\n",
        "print('평균 길이 :', round(sum(map(len, df['AFTER_BODY'])) / len(df['AFTER_BODY'])))\n",
        "\n",
        "plt.hist([len(s) for s in df['AFTER_BODY']], bins=20)\n",
        "plt.xlim(0,1500)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot([len(s) for s in df['AFTER_BODY']], shade=True)\n",
        "plt.title('Kernel Density Estimation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgTPmY2euj4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training.csv', encoding='cp949')\n",
        "\n",
        "before = []\n",
        "after= []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    before_sentences = []\n",
        "    after_sentences = []\n",
        "\n",
        "    for s in A:\n",
        "        before_sentences.append(sent_tokenize(df.iloc[i,2]))\n",
        "        after_sentences.append(sent_tokenize(df.iloc[i,4]))\n",
        "\n",
        "    before.append(len(before_sentences[0]))\n",
        "    after.append(len(after_sentences[0]))\n",
        "\n",
        "print('최대 길이 :', max(before))\n",
        "print('평균 길이 :', sum(before)/len(before))\n",
        "\n",
        "plt.hist(before, bins=20)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "print('최대 길이 :', max(after))\n",
        "print('평균 길이 :', sum(after)/len(after))\n",
        "\n",
        "plt.hist(after, bins=20)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "all = before + after\n",
        "\n",
        "print('최대 길이 :', max(all))\n",
        "print('평균 길이 :', sum(all)/len(all))\n",
        "\n",
        "plt.hist(all, bins=20)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ7zAe69hEez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pxFiN0XYgk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "A = [\"Changes in weather affect near-term gas prices, and a mild summer this year may be followed by a warmer-than-average winter, which could pressure prices. Longer-term, the abundant supply of U.S. natural gas will require an additional demand outlet to soak up incremental output to sustain or raise prices. The likely emergence of exports of liquefied natural gas to Asia may satisfy that need. Industrial growth in the U.S. may raise natural gas consumption as well.\"]\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for s in A:\n",
        "    sentences.append(sent_tokenize(s))\n",
        "\n",
        "sentences = [y for x in sentences for y in x]\n",
        "\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpKnos2DfKTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "clean_sentences = [s.lower() for s in clean_sentences]\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
        "\n",
        "print(clean_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKxQjzoEfHq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings = {}\n",
        "\n",
        "f = open('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/glove.6B.100d.txt', encoding='utf-8')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qBwrRSNfwT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_vectors = []\n",
        "\n",
        "for i in clean_sentences:\n",
        "    if len(i) != 0:\n",
        "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "    else:\n",
        "        v = np.zeros((100,))\n",
        "\n",
        "    sentence_vectors.append(v)\n",
        "\n",
        "print(sentence_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysUz_msxf_Al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(len(sentences)):\n",
        "        if i != j:\n",
        "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORac6usMgOei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XwWvWhfgOXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "for i in range(3):\n",
        "    print(ranked_sentences[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "442rYEvm6cHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_path = r'/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training.csv'\n",
        "\n",
        "dataset = pd.read_csv(dataset_path, encoding='cp949')\n",
        "dataset = dataset.sample(n=50000, random_state=42)\n",
        "\n",
        "df = pd.DataFrame(columns=['x','y'],index=range(100000))\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    df.iloc[i,0] = dataset.iloc[i,2]+\" \"+dataset.iloc[i,4]\n",
        "    df.iloc[50000+i,0] = dataset.iloc[i,4]+\" \"+dataset.iloc[i,2]\n",
        "    df.iloc[i,1] = True\n",
        "    df.iloc[50000+i,1] = False\n",
        "\n",
        "df.to_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0.csv', encoding='cp949', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaeMdPvuVOwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0.csv', encoding='cp949')\n",
        "\n",
        "print(df.dtypes)\n",
        "\n",
        "print('개수 :', len(df))\n",
        "print('최대 길이 :', max(len(l) for l in df['x']))\n",
        "print('평균 길이 :', round(sum(map(len, df['x'])) / len(df['x'])))\n",
        "\n",
        "plt.hist([len(s) for s in df['x']], bins=20)\n",
        "plt.xlim(0,3000)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot([len(s) for s in df['x']], shade=True)\n",
        "plt.title('Kernel Density Estimation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYiU-MxyX8a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Reshape, Conv2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder; LE = LabelEncoder()\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def convert_to_ord(data):\n",
        "    try:\n",
        "        return [ord(xx) for xx in data]\n",
        "    except:\n",
        "        print(data)\n",
        "\n",
        "def conv2d_cnn():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=2**16, output_dim=output_dim, input_length=max_len))\n",
        "    model.add(Reshape((max_len, output_dim, 1), input_shape=(max_len, output_dim)))\n",
        "    model.add(Conv2D(filters=filters, kernel_size=(kernel_size, output_dim), strides=(1, 1), padding='valid'))\n",
        "    model.add(GlobalMaxPooling2D())\n",
        "\n",
        "    model.add(Dense(2**6))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(2))\n",
        "    model.add(Activation('softmax'))\n",
        "    adam = optimizers.Adam(lr=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    early_stopping = EarlyStopping(patience=10)\n",
        "    history = model.fit(x_train, y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "    return model, history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    max_len = 3000\n",
        "    output_dim = 200\n",
        "    filters = 400\n",
        "    kernel_size = 5\n",
        "    epochs = 2**7\n",
        "    batch_size = 2**15\n",
        "\n",
        "    dataset_path = r'/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0.csv'\n",
        "\n",
        "    dataset = pd.read_csv(dataset_path, encoding='cp949')\n",
        "\n",
        "    dataset['x'] = dataset['x'].map(convert_to_ord)\n",
        "    dataset['y'] = LE.fit_transform(dataset['y'])\n",
        "\n",
        "    data = sequence.pad_sequences(dataset['x'], maxlen=max_len)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(data, dataset['y'], test_size=0.1, random_state=42)\n",
        "\n",
        "    print('train_shape : {} / {}'.format(x_train.shape, y_train.shape))\n",
        "    print('test_shape : {} / {}'.format(x_test.shape, y_test.shape))\n",
        "\n",
        "    y_true = copy.deepcopy(y_test)\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    model, history = conv2d_cnn()\n",
        "\n",
        "    #model.save('/gdrive/My Drive/TEST/model.h5')\n",
        "\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    print(scores)\n",
        "    print(\"정확도: %.2f%%\" % (scores[1] * 100))\n",
        "\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    y_true = list(y_true)\n",
        "    y_pred = model.predict_classes(x_test)\n",
        "    y_pred = list(y_pred)\n",
        "\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(pd.crosstab(pd.Series(y_true), pd.Series(y_pred), rownames=['True'], colnames=['Predicted']))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}