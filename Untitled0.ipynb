{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/7+agp8pZXJ8MKr8fhyIH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skaurl/HAAFOR-Challenge-2020/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQwVZNuyeIFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aJpQZYmeXlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training.csv', encoding='cp949')\n",
        "\n",
        "print(df.dtypes)\n",
        "\n",
        "print('개수 :', len(df))\n",
        "print('최대 길이 :', max(len(l) for l in df['BEFORE_BODY']))\n",
        "print('평균 길이 :', round(sum(map(len, df['BEFORE_BODY'])) / len(df['BEFORE_BODY'])))\n",
        "\n",
        "plt.hist([len(s) for s in df['BEFORE_BODY']], bins=20)\n",
        "plt.xlim(0,1500)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot([len(s) for s in df['BEFORE_BODY']], shade=True)\n",
        "plt.title('Kernel Density Estimation')\n",
        "plt.show()\n",
        "\n",
        "print('개수 :', len(df))\n",
        "print('최대 길이 :', max(len(l) for l in df['AFTER_BODY']))\n",
        "print('평균 길이 :', round(sum(map(len, df['AFTER_BODY'])) / len(df['AFTER_BODY'])))\n",
        "\n",
        "plt.hist([len(s) for s in df['AFTER_BODY']], bins=20)\n",
        "plt.xlim(0,1500)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot([len(s) for s in df['AFTER_BODY']], shade=True)\n",
        "plt.title('Kernel Density Estimation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgTPmY2euj4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training.csv', encoding='cp949')\n",
        "\n",
        "before = []\n",
        "after= []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    before_sentences = []\n",
        "    after_sentences = []\n",
        "\n",
        "    for s in df.iloc[i,2]:\n",
        "        before_sentences.append(sent_tokenize(df.iloc[i,2]))\n",
        "    for s in df.iloc[i,4]:\n",
        "        after_sentences.append(sent_tokenize(df.iloc[i,4]))\n",
        "\n",
        "    before.append(len(before_sentences[0]))\n",
        "    after.append(len(after_sentences[0]))\n",
        "\n",
        "print('최대 길이 :', max(before))\n",
        "print('평균 길이 :', sum(before)/len(before))\n",
        "\n",
        "plt.hist(before, bins=20)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "print('최대 길이 :', max(after))\n",
        "print('평균 길이 :', sum(after)/len(after))\n",
        "\n",
        "plt.hist(after, bins=20)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "all = before + after\n",
        "\n",
        "print('최대 길이 :', max(all))\n",
        "print('평균 길이 :', sum(all)/len(all))\n",
        "\n",
        "plt.hist(all, bins=20)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKxQjzoEfHq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "word_embeddings = {}\n",
        "\n",
        "f = open('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/glove.6B.100d.txt', encoding='utf-8')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ7zAe69hEez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training.csv', encoding='cp949')\n",
        "df = df.dropna(how = 'any')\n",
        "\n",
        "for i in tqdm(range(len(df))):\n",
        "    try:\n",
        "        sentences = []\n",
        "        sentences.append(sent_tokenize(df.iloc[i,2]))\n",
        "        sentences = sentences[0]\n",
        "        if len(sentences) >= 3:\n",
        "            clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "            clean_sentences = [s.lower() for s in clean_sentences]\n",
        "            stop_words = stopwords.words('english')\n",
        "            clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
        "            sentence_vectors = []\n",
        "            for j in clean_sentences:\n",
        "                if len(j) != 0:\n",
        "                    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in j.split()])/(len(j.split())+0.001)\n",
        "                else:\n",
        "                    v = np.zeros((100,))\n",
        "                sentence_vectors.append(v)\n",
        "            sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "            for a in range(len(sentences)):\n",
        "                for b in range(len(sentences)):\n",
        "                    if a != b:\n",
        "                        sim_mat[a][b] = cosine_similarity(sentence_vectors[a].reshape(1,100), sentence_vectors[b].reshape(1,100))[0,0]\n",
        "            nx_graph = nx.from_numpy_array(sim_mat)\n",
        "            scores = nx.pagerank(nx_graph)\n",
        "            ranked_sentences = sorted(((scores[a],b) for a,b in enumerate(sentences)), reverse=True)\n",
        "            df.iloc[i,2] = ranked_sentences[0][1]+\" \"+ranked_sentences[1][1]+\" \"+ranked_sentences[2][1]\n",
        "        else:\n",
        "            pass\n",
        "    except:\n",
        "        df.iloc[i,2] = np.nan\n",
        "\n",
        "df = df.dropna(how = 'any')\n",
        "df.to_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0.csv', encoding='cp949', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_AJka9x9rC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0.csv', encoding='cp949')\n",
        "df = df.dropna(how = 'any')\n",
        "\n",
        "for i in tqdm(range(len(df))):\n",
        "    try:\n",
        "        sentences = []\n",
        "        sentences.append(sent_tokenize(df.iloc[i,4]))\n",
        "        sentences = sentences[0]\n",
        "        if len(sentences) >= 3:\n",
        "            clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "            clean_sentences = [s.lower() for s in clean_sentences]\n",
        "            stop_words = stopwords.words('english')\n",
        "            clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
        "            sentence_vectors = []\n",
        "            for j in clean_sentences:\n",
        "                if len(j) != 0:\n",
        "                    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in j.split()])/(len(j.split())+0.001)\n",
        "                else:\n",
        "                    v = np.zeros((100,))\n",
        "                sentence_vectors.append(v)\n",
        "            sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "            for a in range(len(sentences)):\n",
        "                for b in range(len(sentences)):\n",
        "                    if a != b:\n",
        "                        sim_mat[a][b] = cosine_similarity(sentence_vectors[a].reshape(1,100), sentence_vectors[b].reshape(1,100))[0,0]\n",
        "            nx_graph = nx.from_numpy_array(sim_mat)\n",
        "            scores = nx.pagerank(nx_graph)\n",
        "            ranked_sentences = sorted(((scores[a],b) for a,b in enumerate(sentences)), reverse=True)\n",
        "            df.iloc[i,4] = ranked_sentences[0][1]+\" \"+ranked_sentences[1][1]+\" \"+ranked_sentences[2][1]\n",
        "        else:\n",
        "            pass\n",
        "    except:\n",
        "        df.iloc[i,4] = np.nan\n",
        "\n",
        "df = df.dropna(how = 'any')\n",
        "df.to_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0.csv', encoding='cp949', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNodkCnW_KHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0.csv', encoding='cp949')\n",
        "\n",
        "print(df.dtypes)\n",
        "\n",
        "print('개수 :', len(df))\n",
        "print('최대 길이 :', max(len(l) for l in df['BEFORE_BODY']))\n",
        "print('평균 길이 :', round(sum(map(len, df['BEFORE_BODY'])) / len(df['BEFORE_BODY'])))\n",
        "\n",
        "plt.hist([len(s) for s in df['BEFORE_BODY']], bins=20)\n",
        "plt.xlim(0,1500)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot([len(s) for s in df['BEFORE_BODY']], shade=True)\n",
        "plt.title('Kernel Density Estimation')\n",
        "plt.show()\n",
        "\n",
        "print('개수 :', len(df))\n",
        "print('최대 길이 :', max(len(l) for l in df['AFTER_BODY']))\n",
        "print('평균 길이 :', round(sum(map(len, df['AFTER_BODY'])) / len(df['AFTER_BODY'])))\n",
        "\n",
        "plt.hist([len(s) for s in df['AFTER_BODY']], bins=20)\n",
        "plt.xlim(0,1500)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot([len(s) for s in df['AFTER_BODY']], shade=True)\n",
        "plt.title('Kernel Density Estimation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H01cZvIi_P94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0.csv', encoding='cp949')\n",
        "\n",
        "before = []\n",
        "after= []\n",
        "\n",
        "for i in tqdm(range(len(df))):\n",
        "    before_sentences = []\n",
        "    after_sentences = []\n",
        "\n",
        "    for s in df.iloc[i,2]:\n",
        "        before_sentences.append(sent_tokenize(df.iloc[i,2]))\n",
        "    for s in df.iloc[i,4]:\n",
        "        after_sentences.append(sent_tokenize(df.iloc[i,4]))\n",
        "\n",
        "    before.append(len(before_sentences[0]))\n",
        "    after.append(len(after_sentences[0]))\n",
        "\n",
        "print('최대 길이 :', max(before))\n",
        "print('평균 길이 :', sum(before)/len(before))\n",
        "\n",
        "plt.hist(before, bins=20)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "print('최대 길이 :', max(after))\n",
        "print('평균 길이 :', sum(after)/len(after))\n",
        "\n",
        "plt.hist(after, bins=20)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "all = before + after\n",
        "\n",
        "print('최대 길이 :', max(all))\n",
        "print('평균 길이 :', sum(all)/len(all))\n",
        "\n",
        "plt.hist(all, bins=20)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "442rYEvm6cHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "dataset_path = r'/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0.csv'\n",
        "\n",
        "dataset = pd.read_csv(dataset_path, encoding='cp949')\n",
        "\n",
        "length = int(len(dataset)/2)\n",
        "\n",
        "df = pd.DataFrame(columns=['x','y'],index=range(length*2))\n",
        "\n",
        "for i in tqdm(range(length)):\n",
        "    df.iloc[i,0] = dataset.iloc[i,2]+\" \"+dataset.iloc[i,4]\n",
        "    df.iloc[length+i,0] = dataset.iloc[i,4]+\" \"+dataset.iloc[i,2]\n",
        "    df.iloc[i,1] = True\n",
        "    df.iloc[length+i,1] = False\n",
        "\n",
        "df.to_csv('/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0_1.csv', encoding='cp949', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK9k1uY9I-tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "dataset_path = r'/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0_1.csv'\n",
        "\n",
        "dataset = pd.read_csv(dataset_path, encoding='cp949')\n",
        "\n",
        "print(df.dtypes)\n",
        "\n",
        "print('개수 :', len(df))\n",
        "print('최대 길이 :', max(len(l) for l in df['x']))\n",
        "print('평균 길이 :', round(sum(map(len, df['x'])) / len(df['x'])))\n",
        "\n",
        "plt.hist([len(s) for s in df['x']], bins=20)\n",
        "plt.xlim(0,2000)\n",
        "plt.xlabel('length of Data')\n",
        "plt.ylabel('number of Data')\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot([len(s) for s in df['x']], shade=True)\n",
        "plt.title('Kernel Density Estimation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYiU-MxyX8a-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "6fcc4f1f-a554-4d82-9eb5-8541b97f4e59"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Reshape, Conv2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder; LE = LabelEncoder()\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def convert_to_ord(data):\n",
        "    try:\n",
        "        return [ord(xx) for xx in data]\n",
        "    except:\n",
        "        print(data)\n",
        "\n",
        "def conv2d_cnn():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=2**16, output_dim=output_dim, input_length=max_len))\n",
        "    model.add(Reshape((max_len, output_dim, 1), input_shape=(max_len, output_dim)))\n",
        "    model.add(Conv2D(filters=filters, kernel_size=(kernel_size, output_dim), strides=(1, 1), padding='valid'))\n",
        "    model.add(GlobalMaxPooling2D())\n",
        "\n",
        "    model.add(Dense(2**6))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(2))\n",
        "    model.add(Activation('softmax'))\n",
        "    adam = optimizers.Adam(lr=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    early_stopping = EarlyStopping(patience=10)\n",
        "    history = model.fit(x_train, y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "    return model, history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    max_len = 1500\n",
        "    output_dim = 200\n",
        "    filters = 400\n",
        "    kernel_size = 5\n",
        "    epochs = 2**7\n",
        "    batch_size = 2**15\n",
        "\n",
        "    dataset_path = r'/gdrive/My Drive/한양대학교/HAAFOR Challenge 2020/training_untitled0_1.csv'\n",
        "\n",
        "    dataset = pd.read_csv(dataset_path, encoding='cp949')\n",
        "\n",
        "    dataset['x'] = dataset['x'].map(convert_to_ord)\n",
        "    dataset['y'] = LE.fit_transform(dataset['y'])\n",
        "\n",
        "    data = sequence.pad_sequences(dataset['x'], maxlen=max_len)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(data, dataset['y'], test_size=0.1, random_state=42)\n",
        "\n",
        "    print('train_shape : {} / {}'.format(x_train.shape, y_train.shape))\n",
        "    print('test_shape : {} / {}'.format(x_test.shape, y_test.shape))\n",
        "\n",
        "    y_true = copy.deepcopy(y_test)\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "    model, history = conv2d_cnn()\n",
        "\n",
        "    #model.save('/gdrive/My Drive/TEST/model.h5')\n",
        "\n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    print(scores)\n",
        "    print(\"정확도: %.2f%%\" % (scores[1] * 100))\n",
        "\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    y_true = list(y_true)\n",
        "    y_pred = model.predict_classes(x_test)\n",
        "    y_pred = list(y_pred)\n",
        "\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(pd.crosstab(pd.Series(y_true), pd.Series(y_pred), rownames=['True'], colnames=['Predicted']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_shape : (179800, 1500) / (179800,)\n",
            "test_shape : (19978, 1500) / (19978,)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1500, 200)         13107200  \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 1500, 200, 1)      0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 1496, 1, 400)      400400    \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d (Global (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                25664     \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 130       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 13,533,394\n",
            "Trainable params: 13,533,394\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/128\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}